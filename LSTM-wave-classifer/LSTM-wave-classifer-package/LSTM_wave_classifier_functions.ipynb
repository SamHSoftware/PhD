{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "\n",
    "# A function to allow the user to select the folder contianing the data.\n",
    "# Function inputs args: None. \n",
    "# Function output 1: The path of that the folder selected by the user. \n",
    "def folder_selection_dialog():\n",
    "    root = Tk()\n",
    "    root.title('Please select the directory containing the .xlsx files')\n",
    "    root.filename = filedialog.askdirectory(initialdir=\"/\", title=\"Select A Folder\")\n",
    "    directory = root.filename\n",
    "    root.destroy()\n",
    "\n",
    "    return directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Function inputs arg 1: num_epochs --> The number of iterations over which the model is refined. \n",
    "# Function inputs arg 2: training_loss --> Array of size 1 x num_epochs. This array contains the calculated values of loss for training. \n",
    "# Function inputs arg 3: validation_loss --> Array of size 1 x num_epochs. This array contains the calculated values of loss for validation. \n",
    "# Function inputs arg 4: save_plot --> True or Flase. When true, saves plot to data directory.  \n",
    "# Function inputs arg 5: display_plot --> True or Flase. When true, displays the plot. \n",
    "# Function output: Graph with the loss per epoch.\n",
    "def loss_graph(num_epochs, \n",
    "               training_loss, \n",
    "               validation_loss, \n",
    "               save_plot, \n",
    "               display_plot):\n",
    "    \n",
    "    # Plot the loss per epoch. \n",
    "    y = list(range(0,num_epochs))\n",
    "    plt.plot(y, training_loss, label=\"Training loss\")\n",
    "    plt.plot(y, validation_loss, label=\"Validation loss\")\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    plt.ylabel('BCE calculated loss', labelpad=10) # The leftpad argument alters the distance of the axis label from the axis itself. \n",
    "    plt.xlabel('Epoch', labelpad=10)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    # Save the plot if the user desires it.\n",
    "    if save_plot:\n",
    "        current_directory = os.getcwd()\n",
    "        file_path, _ = os.path.split(current_directory)\n",
    "        file_path = os.path.join(file_path, 'img', 'training_and_validation_loss.png')\n",
    "        plt.savefig(file_path, dpi=200, bbox_inches='tight')\n",
    "    \n",
    "    # Display the plot if the user desires it. \n",
    "    if (display_plot == False):\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Function inputs arg 1: num_epochs --> The number of iterations over which the model is refined. \n",
    "# Function inputs arg 2: training_accuracy --> Array of size 1 x num_epochs. This array contains the calculated values of training accuracy. \n",
    "# Function inputs arg 3: validation_accuracy --> Array of size 1 x num_epochs. This array contains the calculated values of validation accuracy. \n",
    "# Function inputs arg 4: save_plot --> True or Flase. When true, saves plot to data directory.  \n",
    "# Function inputs arg 5: display_plot --> True or Flase. When true, displays the plot. \n",
    "# Function output: Graph with the training and validation accuracy per epoch.\n",
    "def accuracy_graph(num_epochs, \n",
    "               training_accuracy, \n",
    "               validation_accuracy, \n",
    "               save_plot, \n",
    "               display_plot):\n",
    "    \n",
    "    # Plot the BCE calculated loss per epoch. \n",
    "    y = list(range(0,num_epochs))\n",
    "    plt.plot(y, training_accuracy, label=\"Training accuracy\")\n",
    "    plt.plot(y, validation_accuracy, label=\"Validation accuracy\")\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    plt.ylabel('Accuracy', labelpad=10) # The leftpad argument alters the distance of the axis label from the axis itself. \n",
    "    plt.xlabel('Epoch', labelpad=10)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    # Save the plot if the user desires it.\n",
    "    if save_plot:\n",
    "        current_directory = os.getcwd()\n",
    "        file_path, _ = os.path.split(current_directory)\n",
    "        file_path = os.path.join(file_path, 'img', 'training_and_validation_accuracy.png')\n",
    "        plt.savefig(file_path, dpi=200, bbox_inches='tight')\n",
    "    \n",
    "    # Display the plot if the user desires it. \n",
    "    if (display_plot == False):\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# This function creates a confusion matrix to help assess the model. \n",
    "# Function inputs arg 1: cm --> The confusion matrix as generated by the function 'confusion_matrix()'\n",
    "# Function inputs arg 2: classes --> Tuple of strings to label class identities on the plot.  \n",
    "# Function inputs arg 3: normalize --> True or Flase. When true, data is normalized between 0 and 1 relative to the total of each row.\n",
    "# Function inputs arg 4: title --> A string. \n",
    "# Function inputs arg 5: cmap --> The chosen colormap. \n",
    "# Function inputs arg 6: save_plot --> True or Flase. When true, saves plot to data directory.  \n",
    "# Function inputs arg 7: display_plot --> True or Flase. When true, displays the plot. \n",
    "# Function output: Figure with the confusion matrix. \n",
    "def plot_confusion_matrix(cm, \n",
    "                          classes,\n",
    "                          save_plot=True,\n",
    "                          display_plot=True,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    \n",
    "    # Save the plot if the user desires it.\n",
    "    if save_plot:\n",
    "        current_directory = os.getcwd()\n",
    "        file_path, _ = os.path.split(current_directory)\n",
    "        file_path = os.path.join(file_path, 'img', 'confusion_matrix.png')\n",
    "        plt.savefig(file_path, dpi=200, bbox_inches='tight')\n",
    "    \n",
    "    # Display the plot if the user desires it. \n",
    "    if (display_plot == False):\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "# A function to extract and condense the relevant data. \n",
    "# Function input arg 1: directory (string) --> The directory to the folder containing the .xlsx data.\n",
    "# Funciton output 1: df --> The pandas dataframe containing the training information.\n",
    "def get_red_waves(directory):\n",
    "    \n",
    "    # Get a list of the .xlsx files. \n",
    "    files = [_ for _ in os.listdir(directory) if _.endswith('.xlsx')]\n",
    "\n",
    "    # Create a list to hold the data. \n",
    "    df = []\n",
    "\n",
    "    # Loop through the individual .xlsx files and extract the 'red' information. \n",
    "    for t in range(len(files)):\n",
    "\n",
    "        # Construct the file path. \n",
    "        file_t = os.path.join(directory, files[t])\n",
    "        \n",
    "        # Load in the .xlsx data. \n",
    "        data = pd.read_excel(file_t, index_col=None)\n",
    "        \n",
    "        # Delete all rows which are not of an even timepoint (hours). This is to standardize the time interval.\n",
    "        indices = 1 - data.iloc[:,2] % 1\n",
    "        data = data[indices == 1]\n",
    "        \n",
    "        # Append the red data, the file name and the binary wave value. \n",
    "        data = data.iloc[:,6]\n",
    "        data = data.values.tolist()\n",
    "        data.insert(0, files[t])\n",
    "        \n",
    "        wave_value = re.search('_(\\d)\\.', files[t]).group(1)\n",
    "        data.insert(0, wave_value)\n",
    "        \n",
    "        # Aoppend the list of information to our array as a new row. \n",
    "        df.append(data)\n",
    "    \n",
    "    # Convert our array to a pandas dataframe. \n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# A function to train an LSTM model to recognise repetitive sequence data as having waves or not. \n",
    "# Function inputs arg 1: df --> pandas dataframe as provided by the 'get_red_waevs()' function. \n",
    "# Function inputs arg 2: save_plot --> True or False. When True, saves plot to the img folder of the package. \n",
    "# Function inputs arg 3: display_plot --> True or False. When True, displays plot within conole. \n",
    "# Function output 1: The trained model.\n",
    "def train_LSTM(df,\n",
    "               save_plot = True,\n",
    "               display_plot = True):\n",
    "\n",
    "    ##### (1) Use the computer graphics card if one is available. \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    ##### (2) Load and prepare the data. \n",
    "    y = df.iloc[:, 0]\n",
    "    x = df.iloc[:, 2:len(df.columns)]\n",
    "    \n",
    "    # Split the data into testing and training data. \n",
    "    x_training, x_testing, y_training, y_testing = train_test_split(x, y, test_size = 0.2) # Use random_state = 1234 to create the same split for unit testing. \n",
    "    \n",
    "    # Create a weight tensor to deal with imbalanced classes. \n",
    "    wave_count = y_training.astype(float).sum()\n",
    "    noWave_count = y_training.shape[0] - wave_count\n",
    "    weights = torch.tensor(max(wave_count, noWave_count) / [wave_count, noWave_count])\n",
    "    \n",
    "    # Transpose all the data, so that the LSTM correctly understands that each column is a sequence. \n",
    "    x_training = x_training.transpose()\n",
    "    x_testing = x_testing.transpose()\n",
    "    y_training = y_training.transpose()\n",
    "    y_testing = y_testing.transpose()\n",
    "    \n",
    "    # Scale the data by 'removign the mean and scaling to unit variance'.\n",
    "    x_training = StandardScaler().fit_transform(x_training)\n",
    "    x_testing = StandardScaler().fit_transform(x_testing)\n",
    "    \n",
    "    # Convert the data to tensors. \n",
    "    x_training = torch.from_numpy(x_training.astype(np.float32))\n",
    "    x_testing = torch.from_numpy(x_testing.astype(np.float32))\n",
    "    \n",
    "    y_training = np.array(y_training)\n",
    "    y_training = torch.from_numpy(y_training.astype(np.float32))\n",
    "    opposite_tensor = torch.ones(y_training.shape[0]) - y_training\n",
    "    y_training = torch.stack([y_training, opposite_tensor], dim=1)\n",
    "    \n",
    "    y_testing = np.array(y_testing)\n",
    "    y_testing = torch.from_numpy(y_testing.astype(np.float32))\n",
    "    opposite_tensor = torch.ones(y_testing.shape[0]) - y_testing\n",
    "    y_testing = torch.stack([y_testing, opposite_tensor], dim=1) \n",
    "    \n",
    "    ##### (3) Create the LSTM model. \n",
    "    \n",
    "    class LSTM_model(nn.Module):\n",
    "    \n",
    "        # Constructor: \n",
    "        def __init__(self, input_size, num_layers, hidden_size = 2):\n",
    "            super(LSTM_model, self).__init__()\n",
    "            \n",
    "            self.input_size = input_size # The number of features per timepoint. We have one feature/number, the red proportion. \n",
    "            self.hidden_size = hidden_size # This functions to reduce the dimensionality of the data input_size. Thus, keep it at as a value of 1... we can't go lower! \n",
    "            self.num_layers = num_layers # The number of layers, NOT the number of LSTM cells, which will implicitly be set to equal the sequence length. \n",
    "            self.lstm = nn.LSTM(input_size, \n",
    "                                hidden_size, \n",
    "                                num_layers) # This is the LSTM layer. \n",
    "\n",
    "            self.linear = nn.Linear(in_features=hidden_size, \n",
    "                                    out_features=2)\n",
    "            \n",
    "            self.softmax = nn.Softmax(dim=1) # This is to binarize our outputs. \n",
    "              \n",
    "        # Define the forward pass. \n",
    "        def forward(self, input): \n",
    "            \n",
    "            #Initialise the hidden and cell states. \n",
    "            h0 = torch.zeros(self.num_layers, input.shape[1], self.hidden_size)\n",
    "            c0 = torch.zeros(self.num_layers, input.shape[1], self.hidden_size)\n",
    "            \n",
    "            # Pytorch LSTM requires a 3D tensor. \n",
    "            input = input.view(input.shape[0], input.shape[1], 1) \n",
    "            \n",
    "            # Pass our input into our LSTM.\n",
    "            lstm_out, _ = self.lstm(input, (h0, c0))\n",
    "            \n",
    "            # We pass the last output from the LSTM, not the entire LSTM output.\n",
    "            y_pred = self.softmax(self.linear(lstm_out[-1, :, :])) # -1 indicates the 'end' of the dimension.\n",
    "            return y_pred      \n",
    "    \n",
    "    #### (4) Create an instance of the model. \n",
    "    model = LSTM_model(input_size = 1, num_layers = 5, hidden_size = 5).to(device)\n",
    "    \n",
    "    #### (5) Loss and optimizer. \n",
    "    calc_loss = nn.BCELoss(weight=weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    #### (6) Training loop.\n",
    "    num_epochs = 100\n",
    "    training_loss = [] \n",
    "    validation_loss = []\n",
    "    training_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            completion = (epoch / num_epochs)*100\n",
    "            print(completion)\n",
    "    \n",
    "        # Iterate through batches of training data to reduce training time. \n",
    "        permutation = torch.randperm(x_training.size()[0])\n",
    "        batch_size = 16\n",
    "        for batch in range(0,x_training.size()[0], batch_size):\n",
    "            \n",
    "            # Create the batch using randomly assigned indices. \n",
    "            indices = permutation[batch:batch+batch_size]\n",
    "            batch_x_training = x_training[indices, :]\n",
    "            batch_y_training = y_training[indices, :]\n",
    "    \n",
    "            # Ensure that the model calculates gradients. \n",
    "            model.train()\n",
    "\n",
    "            # Forward pass: compute the output of the layers given the input sequences. \n",
    "            y_training_predicted = model(batch_x_training)\n",
    "\n",
    "            # Log the training loss per epoch.\n",
    "            # Input the ars as the 'target' followed by the 'output'. The order is important! \n",
    "            loss = calc_loss(y_training_predicted, batch_y_training)\n",
    "            loss_value = loss.detach().numpy()\n",
    "            loss_value = loss_value.item()\n",
    "            training_loss.append(loss_value)\n",
    "\n",
    "            # Log the training accuracy per epoch. \n",
    "            y_training_predicted_classes = y_training_predicted[:,0].round()\n",
    "            accuracy = y_training_predicted_classes.eq(y_training[:,0]).sum().detach().numpy() / float(y_training.shape[0])\n",
    "            training_accuracy.append(accuracy)\n",
    "\n",
    "            # Zero the gradients to prevent their cumulative build-up per epoch. \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass. Calculate d loss/d x. This is the gradient claculation per weight. \n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights.\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Log the loss and accuracy for the overall (not the batch) training and testing set. \n",
    "        # Disable the dropout while we perform model validation. \n",
    "        model.eval()\n",
    "\n",
    "        # torch.no_grad() disables gradient calucation. We don't neet it for validation. \n",
    "        with torch.no_grad():\n",
    "            # Forward pass: compute the output of the layers given the input sequences. \n",
    "            y_testing_predicted = model(x_testing)\n",
    "\n",
    "            # Log the validation loss per epoch. \n",
    "            # Input the ars as the 'target' followed by the 'output'. The order is improtant! \n",
    "            y_testing_predicted = model(x_testing)\n",
    "            loss = calc_loss(y_testing_predicted, y_testing)\n",
    "            loss_value = loss.detach().numpy()\n",
    "            loss_value = loss_value.item()\n",
    "            validation_loss.append(loss_value)\n",
    "\n",
    "            # Log the validation accuracy per epoch. \n",
    "            y_testing_predicted_classes = y_testing_predicted[:,0].round()\n",
    "            accuracy = y_testing_predicted_classes.eq(y_testing[:,0]).sum().detach().numpy() / float(y_testing.shape[0])\n",
    "            validation_accuracy.append(accuracy)\n",
    "\n",
    "            # Zero the gradients to prevent their cumulative build-up per epoch. \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    ##### (7) Plot the data associated with the training of our LSTM model.\n",
    "    \n",
    "    # Plot the loss graph. \n",
    "    loss_graph(num_epochs, \n",
    "               training_loss, \n",
    "               validation_loss, \n",
    "               save_plot, \n",
    "               display_plot)\n",
    "    \n",
    "    # Plot the accuracy graph. \n",
    "    accuracy_graph(num_epochs, \n",
    "               training_accuracy, \n",
    "               validation_accuracy, \n",
    "               save_plot, \n",
    "               display_plot)\n",
    "    \n",
    "    # Plot the confusion matrix.\n",
    "    confusion = confusion_matrix(y_testing[:,0].detach().numpy(), y_testing_predicted_classes.detach().numpy())\n",
    "    names = ('3-4', '1-2')\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusion, \n",
    "                          names, \n",
    "                          save_plot, \n",
    "                          display_plot,\n",
    "                          normalize=False)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# A function to test how the number of training examples affect loss and validation.  \n",
    "# Function inputs arg 1: df --> pandas dataframe as provided by the 'get_red_waevs()' function. \n",
    "# Function inputs arg 2: save_plot --> True or False. When True, saves plot to the img folder of the package. \n",
    "# Function inputs arg 3: display_plot --> True or False. When True, displays plot within conole. \n",
    "# Function output 1: \n",
    "def assess_LSTM_training(df,\n",
    "                         save_plot = True,\n",
    "                         display_plot = True):\n",
    "\n",
    "    ##### (1) Use the computer graphics card if one is available. \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    ##### With each iteration, provide an extra training example to the model. \n",
    "    training_loss = [] \n",
    "    validation_loss = []\n",
    "    for num_examples in range(19, df.shape[0]): \n",
    "        \n",
    "        # Print progress \n",
    "        print(num_examples)\n",
    "        \n",
    "        ##### (2) Load and prepare the data. \n",
    "        y = df.iloc[0:num_examples, 0]\n",
    "        x = df.iloc[0:num_examples, 2:len(df.columns)]\n",
    "        \n",
    "        # Split the data into testing and training data. \n",
    "        x_training, x_testing, y_training, y_testing = train_test_split(x, y, test_size = 0.2) # Use random_state = 1234 to create the same split for unit testing. \n",
    "\n",
    "        # Create a weight tensor to deal with imbalanced classes. \n",
    "        wave_count = y_training.astype(float).sum()\n",
    "        noWave_count = y_training.shape[0] - wave_count\n",
    "        weights = torch.tensor(max(wave_count, noWave_count) / [wave_count, noWave_count])\n",
    "\n",
    "        # Transpose all the data, so that the LSTM correctly understands that each column is a sequence. \n",
    "        x_training = x_training.transpose()\n",
    "        x_testing = x_testing.transpose()\n",
    "        y_training = y_training.transpose()\n",
    "        y_testing = y_testing.transpose()\n",
    "\n",
    "        # Scale the data by 'removign the mean and scaling to unit variance'.\n",
    "        x_training = StandardScaler().fit_transform(x_training)\n",
    "        x_testing = StandardScaler().fit_transform(x_testing)\n",
    "\n",
    "        # Convert the data to tensors. \n",
    "        x_training = torch.from_numpy(x_training.astype(np.float32))\n",
    "        x_testing = torch.from_numpy(x_testing.astype(np.float32))\n",
    "\n",
    "        y_training = np.array(y_training)\n",
    "        y_training = torch.from_numpy(y_training.astype(np.float32))\n",
    "        opposite_tensor = torch.ones(y_training.shape[0]) - y_training\n",
    "        y_training = torch.stack([y_training, opposite_tensor], dim=1)\n",
    "\n",
    "        y_testing = np.array(y_testing)\n",
    "        y_testing = torch.from_numpy(y_testing.astype(np.float32))\n",
    "        opposite_tensor = torch.ones(y_testing.shape[0]) - y_testing\n",
    "        y_testing = torch.stack([y_testing, opposite_tensor], dim=1) \n",
    "\n",
    "        ##### (3) Create the LSTM model. \n",
    "\n",
    "        class LSTM_model(nn.Module):\n",
    "\n",
    "            # Constructor: \n",
    "            def __init__(self, input_size, num_layers, hidden_size = 2):\n",
    "                super(LSTM_model, self).__init__()\n",
    "\n",
    "                self.input_size = input_size # The number of features per timepoint. We have one feature/number, the red proportion. \n",
    "                self.hidden_size = hidden_size # This functions to reduce the dimensionality of the data input_size. Thus, keep it at as a value of 1... we can't go lower! \n",
    "                self.num_layers = num_layers # The number of layers, NOT the number of LSTM cells, which will implicitly be set to equal the sequence length. \n",
    "                self.lstm = nn.LSTM(input_size, \n",
    "                                    hidden_size, \n",
    "                                    num_layers) # This is the LSTM layer. \n",
    "\n",
    "                self.linear = nn.Linear(in_features=hidden_size, \n",
    "                                        out_features=2)\n",
    "\n",
    "                self.softmax = nn.Softmax(dim=1) # This is to binarize our outputs. \n",
    "\n",
    "            # Define the forward pass. \n",
    "            def forward(self, input): \n",
    "\n",
    "                #Initialise the hidden and cell states. \n",
    "                h0 = torch.zeros(self.num_layers, input.shape[1], self.hidden_size)\n",
    "                c0 = torch.zeros(self.num_layers, input.shape[1], self.hidden_size)\n",
    "\n",
    "                # Pytorch LSTM requires a 3D tensor. \n",
    "                input = input.view(input.shape[0], input.shape[1], 1) \n",
    "\n",
    "                # Pass our input into our LSTM.\n",
    "                lstm_out, _ = self.lstm(input, (h0, c0))\n",
    "\n",
    "                # We pass the last output from the LSTM, not the entire LSTM output.\n",
    "                y_pred = self.softmax(self.linear(lstm_out[-1, :, :])) # -1 indicates the 'end' of the dimension.\n",
    "                return y_pred      \n",
    "\n",
    "        #### (4) Create an instance of the model. \n",
    "        model = LSTM_model(input_size = 1, num_layers = 3, hidden_size = 3).to(device)\n",
    "\n",
    "        #### (5) Loss and optimizer. \n",
    "        calc_loss = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, eps=0.001)\n",
    "\n",
    "        #### (6) Training loop.\n",
    "        num_epochs = 500\n",
    "        for epoch in range(num_epochs):\n",
    "            '''\n",
    "            if epoch % 100 == 0:\n",
    "                completion = (epoch / num_epochs)*100\n",
    "                print(completion)\n",
    "            '''\n",
    "            # Ensure that the model calculates gradients. \n",
    "            model.train()\n",
    "\n",
    "            # Forward pass: compute the output of the layers given the input sequences. \n",
    "            y_training_predicted = model(x_training)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss = calc_loss(y_training_predicted, y_training)\n",
    "            \n",
    "            # Zero the gradients to prevent their cumulative build-up per epoch. \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass. Calculate d loss/d x. This is the gradient claculation per weight. \n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Zero the gradients to prevent their cumulative build-up per epoch. \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        ##### (7) Get the values of training and validation loss. \n",
    "        \n",
    "        # Log the training loss.\n",
    "        loss = calc_loss(y_training_predicted, y_training)\n",
    "        loss_value = loss.detach().numpy()\n",
    "        loss_value = loss_value.item()\n",
    "        training_loss.append(loss_value)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Log the validation loss. \n",
    "        model.eval() \n",
    "        with torch.no_grad(): \n",
    "            y_testing_predicted = model(x_testing)\n",
    "            loss = calc_loss(y_testing_predicted, y_testing)\n",
    "            loss_value = loss.detach().numpy()\n",
    "            loss_value = loss_value.item()\n",
    "            validation_loss.append(loss_value)\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "    ##### (8) Plot the training and validation loss. \n",
    "    x = list(range(20, y.shape[0]+1))\n",
    "    plt.plot(x, training_loss, label=\"Training loss\")\n",
    "    plt.plot(x, validation_loss, label=\"Validation loss\")\n",
    "    plt.rcParams.update({'font.size': 15})\n",
    "    plt.ylabel('BCE calculated loss')\n",
    "    plt.xlabel('Number of training examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# A function to take the trained LSTM model, and use it to classify our data. \n",
    "# Function inputs arg 1: LSTM_model [bound method] --> The trained LSTM model from the 'LSTM_model' function. \n",
    "def classify_waves: \n",
    "    1+1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
