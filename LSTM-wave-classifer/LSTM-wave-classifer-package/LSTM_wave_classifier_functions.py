from tkinter import *
from tkinter import filedialog
import matplotlib.pyplot as plt
import os
import itertools
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
import pandas as pd 
import re
import torch
import torch.nn as nn 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler 
from sklearn.preprocessing import MinMaxScaler 
import shutil

# A function to allow the user to select the folder containing the data.
# Function inputs args: None. 
# Function output 1: The path of that the folder selected by the user. 
def folder_selection_dialog():
    root = Tk()
    root.title('Please select the directory containing the .xlsx files')
    root.filename = filedialog.askdirectory(initialdir="/", title="Select A Folder")
    directory = root.filename
    root.destroy()

    return directory

# Function inputs arg 1: num_epochs --> The number of iterations over which the model is refined. 
# Function inputs arg 2: training_loss --> Array of size 1 x num_epochs. This array contains the calculated values of loss for training. 
# Function inputs arg 3: validation_loss --> Array of size 1 x num_epochs. This array contains the calculated values of loss for validation. 
# Function inputs arg 4: save_plot --> True or Flase. When true, saves plot to data directory.  
# Function inputs arg 5: display_plot --> True or Flase. When true, displays the plot. 
# Function output: Graph with the loss per epoch.
def loss_graph(num_epochs, 
               training_loss, 
               validation_loss, 
               save_plot, 
               display_plot):
    
    # Plot the loss per epoch. 
    y = list(range(0,num_epochs))
    plt.plot(y, training_loss, label="Training loss")
    plt.plot(y, validation_loss, label="Validation loss")
    plt.rcParams.update({'font.size': 15})
    plt.ylabel('BCE calculated loss', labelpad=10) # The leftpad argument alters the distance of the axis label from the axis itself. 
    plt.xlabel('Epoch', labelpad=10)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    # Save the plot if the user desires it.
    if save_plot:
        current_directory = os.getcwd()
        file_path, _ = os.path.split(current_directory)
        file_path = os.path.join(file_path, 'img', 'training_and_validation_loss.png')
        plt.savefig(file_path, dpi=200, bbox_inches='tight')
    
    # Display the plot if the user desires it. 
    if (display_plot == False):
        plt.close()
    else:
        plt.show()   

# Function inputs arg 1: num_epochs --> The number of iterations over which the model is refined. 
# Function inputs arg 2: training_accuracy --> Array of size 1 x num_epochs. This array contains the calculated values of training accuracy. 
# Function inputs arg 3: validation_accuracy --> Array of size 1 x num_epochs. This array contains the calculated values of validation accuracy. 
# Function inputs arg 4: save_plot --> True or Flase. When true, saves plot to data directory.  
# Function inputs arg 5: display_plot --> True or Flase. When true, displays the plot. 
# Function output: Graph with the training and validation accuracy per epoch.
def accuracy_graph(num_epochs, 
               training_accuracy, 
               validation_accuracy, 
               save_plot, 
               display_plot):
    
    # Plot the BCE calculated loss per epoch. 
    y = list(range(0,num_epochs))
    plt.plot(y, training_accuracy, label="Training accuracy")
    plt.plot(y, validation_accuracy, label="Validation accuracy")
    plt.rcParams.update({'font.size': 15})
    plt.ylabel('Accuracy', labelpad=10) # The leftpad argument alters the distance of the axis label from the axis itself. 
    plt.xlabel('Epoch', labelpad=10)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

    # Save the plot if the user desires it.
    if save_plot:
        current_directory = os.getcwd()
        file_path, _ = os.path.split(current_directory)
        file_path = os.path.join(file_path, 'img', 'training_and_validation_accuracy.png')
        plt.savefig(file_path, dpi=200, bbox_inches='tight')
    
    # Display the plot if the user desires it. 
    if (display_plot == False):
        plt.close()
    else:
        plt.show()   

# This function creates a confusion matrix to help assess the model. 
# Function inputs arg 1: cm --> The confusion matrix as generated by the function 'confusion_matrix()'
# Function inputs arg 2: classes --> Tuple of strings to label class identities on the plot.  
# Function inputs arg 3: normalize --> True or Flase. When true, data is normalized between 0 and 1 relative to the total of each row.
# Function inputs arg 4: title --> A string. 
# Function inputs arg 5: cmap --> The chosen colormap. 
# Function inputs arg 6: save_plot --> True or Flase. When true, saves plot to data directory.  
# Function inputs arg 7: display_plot --> True or Flase. When true, displays the plot. 
# Function output: Figure with the confusion matrix. 
def plot_confusion_matrix(cm, 
                          classes,
                          save_plot=True,
                          display_plot=True,
                          normalize=True,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True')
    plt.xlabel('Predicted')
    
    # Save the plot if the user desires it.
    if save_plot:
        current_directory = os.getcwd()
        file_path, _ = os.path.split(current_directory)
        file_path = os.path.join(file_path, 'img', 'confusion_matrix.png')
        plt.savefig(file_path, dpi=200, bbox_inches='tight')
    
    # Display the plot if the user desires it. 
    if (display_plot == False):
        plt.close()
    else:
        plt.show()   

# A function to extract and condense the relevant data. 
# Function input arg 1: directory (string) --> The directory to the folder containing the .xlsx data.
# Function input arg 2: train_or_classify (string) --> Use 'train', when collecting data to train the model. Use 'classify' when collecting data which needs to be classified.
# Function output 1: df --> The pandas dataframe containing the training information.
def get_red_waves(directory,
                 train_or_classify = 'train'):
    
    # Get a list of the .xlsx files. 
    files = [_ for _ in os.listdir(directory) if _.endswith('.xlsx')]

    # Create a list to hold the data. 
    df = []

    # Loop through the individual .xlsx files and extract the 'red' information. 
    for t in range(len(files)):

        # Construct the file path. 
        file_t = os.path.join(directory, files[t])
        
        # Load in the .xlsx data. 
        data = pd.read_excel(file_t, index_col=None)
        
        # Delete all rows which are not of an even timepoint (hours). This is to standardize the time interval.
        indices = 1 - data.iloc[:,2] % 1
        data = data[indices == 1]
        
        # Append the red data, the file name and the binary wave value. 
        data = data.iloc[:,6]
        data = data.values.tolist()
        data.insert(0, files[t])
        
        if train_or_classify == 'train':
            wave_value = re.search('_(\d)\.', files[t]).group(1)
        elif train_or_classify == 'classify':
            wave_value = 'NA'
        data.insert(0, wave_value)
        
        # Aoppend the list of information to our array as a new row. 
        df.append(data)
    
    # Convert our array to a pandas dataframe. 
    df = pd.DataFrame(df)
    df = df.fillna(0)
    
    return df 

# A function to train an LSTM model to recognise repetitive sequence data as having waves or not. 
# Function inputs arg 1: df --> pandas dataframe as provided by the 'get_red_waevs()' function. 
# Function inputs arg 2: save_plot --> True or False. When True, saves plot to the img folder of the package. 
# Function inputs arg 3: display_plot --> True or False. When True, displays plot within conole. 
# Function output 1: The trained model.
def train_LSTM(df,
               save_plot = True,
               display_plot = True):

    ##### (1) Use the computer graphics card if one is available. 
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    ##### (2) Load and prepare the data. 
    y = df.iloc[:, 0]
    x = df.iloc[:, 2:len(df.columns)]
    
    # Split the data into testing and training data. 
    x_training, x_testing, y_training, y_testing = train_test_split(x, y, test_size = 0.1) # Use random_state = 1234 to create the same split for unit testing. 
    
    # Create a weight tensor to deal with imbalanced classes. 
    wave_count = y_training.astype(float).sum()
    noWave_count = y_training.shape[0] - wave_count
    weights = torch.tensor(max(wave_count, noWave_count) / [wave_count, noWave_count])
    
    # Transpose all the data, so that the LSTM correctly understands that each column is a sequence. 
    x_training = x_training.transpose()
    x_testing = x_testing.transpose()
    y_training = y_training.transpose()
    y_testing = y_testing.transpose()
    
    # Scale the data by 'removing the mean and scaling to unit variance'.
    x_training[x_training.columns] = StandardScaler().fit_transform(x_training[x_training.columns])
    x_testing[x_testing.columns] = StandardScaler().fit_transform(x_testing[x_testing.columns])
    
    # Convert the data to tensors. 
    x_training = torch.from_numpy(x_training.to_numpy(np.float32))
    x_testing = torch.from_numpy(x_testing.to_numpy(np.float32))    
    
    y_training = np.array(y_training)
    y_training = torch.from_numpy(y_training.astype(np.float32))
    opposite_tensor = torch.ones(y_training.shape[0]) - y_training
    y_training = torch.stack([y_training, opposite_tensor], dim=1)
    
    y_testing = np.array(y_testing)
    y_testing = torch.from_numpy(y_testing.astype(np.float32))
    opposite_tensor = torch.ones(y_testing.shape[0]) - y_testing
    y_testing = torch.stack([y_testing, opposite_tensor], dim=1) 
    
    ##### (3) Create the LSTM model. 
    
    class LSTM_model(nn.Module):
    
        # Constructor: 
        def __init__(self, input_size, num_layers, hidden_size = 2):
            super(LSTM_model, self).__init__()
            
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.input_size = input_size # The number of features per timepoint. We have one feature/number, the red proportion. 
            self.hidden_size = hidden_size # This functions to reduce the dimensionality of the data input_size. Thus, keep it at as a value of 1... we can't go lower! 
            self.num_layers = num_layers # The number of layers, NOT the number of LSTM cells, which will implicitly be set to equal the sequence length. 
            self.lstm = nn.LSTM(input_size, 
                                hidden_size, 
                                num_layers,
                                dropout=0.5) # This is the LSTM layer. 

            self.linear = nn.Linear(in_features=hidden_size, 
                                    out_features=2)
            
            self.softmax = nn.Softmax(dim=1) # This is to binarize our outputs. 
              
        # Define the forward pass. 
        def forward(self, input): 
            
            #Initialise the hidden and cell states. 
            h0 = torch.zeros(self.num_layers, input.shape[1], self.hidden_size).to(self.device)
            c0 = torch.zeros(self.num_layers, input.shape[1], self.hidden_size).to(self.device)
            
            # Pytorch LSTM requires a 3D tensor. 
            input = input.view(input.shape[0], input.shape[1], 1) 
            
            # Pass our input into our LSTM.
            lstm_out, _ = self.lstm(input, (h0, c0))
            
            # We pass the last output from the LSTM, not the entire LSTM output.
            y_pred = self.softmax(self.linear(lstm_out[-1, :, :])) # -1 indicates the 'end' of the dimension.
            return y_pred      
    
    #### (4) Create an instance of the model. 
    model = LSTM_model(input_size = 1, num_layers = 4, hidden_size = 4).to(device)
    
    #### (5) Loss and optimizer. 
    calc_loss = nn.BCELoss(weight=weights)
    
    #optimizer = torch.optim.SGD(model.parameters(), lr=0.02) Batch = 16.
    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001, eps=0.001) Batch = 16.
    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01, eps=0.001) Batch = 16.
    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01) Batch = 16.
    # optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # Batch = 16. bad. flatlined quickly. 0.96-0.94.
    # optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # # Batch = 50. loss=0.04. Promising. increase batch size?
    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001, amsgrad=True) # # Batch = 50. loss=0.03. Ehh, maybe increase the trainign group with test train split from 50% to 90%. 
    # Just reduced batch size to 16. loss decrease of 0.3. Waaay better! 
    # Reduced batch size to 10. Was bad. 
    # Gone back to batch of 16, but now it's bad again... maybe I need to look at something else. Perhaps normalise each individual sequence, as opposed to the entire dataset?? 
    # It's not the normalization. 
    # Also, have switched to adamax, loss decrease by 0.1 in 40 epochs! Will decrease lr from 0.001.
    # Tried with batch 100 and lr 0.0001. Starting to work. Need to change the eps though. 
    # eps 0.0001 didn't work. flatlines after 30000 epochs. 
    # eps=0.0000001, still flatlines fr 70000 epochs. 
    # lr=0.001, eps=0.00001 didnt work. flatlined. Going to try a simpler model. 
    # Trying asmgrad to see if it works. Not super impressed so far. 
    optimizer = torch.optim.Adam(model.parameters(), lr=0.002) # # Batch = 50. loss=0.03. Ehh, maybe increase the trainign group with test train split from 50% to 90%. 
    
    #### (6) Training loop.
    num_epochs = 500
    training_loss = [] 
    validation_loss = []
    training_accuracy = []
    validation_accuracy = []
    for epoch in range(num_epochs):
        
        if epoch % 100 == 0:
            completion = (epoch / num_epochs)*100
            print(completion)
    
        # Iterate through batches of training data to reduce training time. 
        permutation = torch.randperm(x_training.size()[1])
        batch_size = 200
        for batch in range(0,x_training.size()[0], batch_size):
            
            # Create the batch using randomly assigned indices. 
            indices = permutation[batch:batch+batch_size]
            batch_x_training = x_training[:, indices]
            batch_y_training = y_training[indices, :]
    
            # Ensure that the model calculates gradients. 
            model.train()

            # Forward pass: compute the output of the layers given the input sequences. 
            y_training_predicted = model(batch_x_training.to(device))

            # Calculate the training loss for the batch.
            # Input the ars as the 'target' followed by the 'output'. The order is important! 
            loss = calc_loss(y_training_predicted.cpu(), batch_y_training)
            
            # Zero the gradients to prevent their cumulative build-up per epoch. 
            optimizer.zero_grad()

            # Backward pass. Calculate d loss/d x. This is the gradient claculation per weight. 
            loss.backward()

            # Update the weights.
            optimizer.step()
        
        # Log the loss and accuracy for the entire (not the batch) training and testing set. 
        model.eval() # Disable the dropout while we perform model validation. 

        # torch.no_grad() disables gradient calucation. We don't neet it for validation. 
        with torch.no_grad():
            
            # ----- TRAINING LOSS/ACCURACY-----
            y_training_predicted = model(x_training.to(device))
            
            # Log the training loss per epoch.
            loss = calc_loss(y_training_predicted.cpu(), y_training)
            loss_value = loss.detach().numpy()
            loss_value = loss_value.item()
            training_loss.append(loss_value)
            
            if epoch % 10 == 0:
                print(loss_value)
            
            # Log the training accuracy per epoch. 
            y_training_predicted_classes = y_training_predicted[:,0].round().cpu()
            accuracy = y_training_predicted_classes.eq(y_training[:,0]).sum().detach().numpy() / float(y_training.shape[0])
            training_accuracy.append(accuracy)
            
            #----- TESTING LOSS/ACCURACY-----
            y_testing_predicted = model(x_testing.to(device))

            # Log the validation loss per epoch. 
            y_testing_predicted = model(x_testing.to(device))
            loss = calc_loss(y_testing_predicted.cpu(), y_testing)
            loss_value = loss.detach().numpy()
            loss_value = loss_value.item()
            validation_loss.append(loss_value)

            # Log the validation accuracy per epoch. 
            y_testing_predicted_classes = y_testing_predicted[:,0].round().cpu()
            accuracy = y_testing_predicted_classes.eq(y_testing[:,0]).sum().detach().numpy() / float(y_testing.shape[0])
            validation_accuracy.append(accuracy)

            # Zero the gradients to prevent their cumulative build-up per epoch. 
            optimizer.zero_grad()

        if epoch % 200 == 0:
            loss_graph(epoch+1, 
               training_loss, 
               validation_loss, 
               save_plot, 
               display_plot)
            
    ##### (7) Plot the data associated with the training of our LSTM model.
    
    # Plot the loss graph. 
    loss_graph(num_epochs, 
               training_loss, 
               validation_loss, 
               save_plot, 
               display_plot)
    
    # Plot the accuracy graph. 
    accuracy_graph(num_epochs, 
               training_accuracy, 
               validation_accuracy, 
               save_plot, 
               display_plot)
    
    # Plot the confusion matrix.
    confusion = confusion_matrix(y_testing[:,0].detach().numpy(), y_testing_predicted_classes.detach().numpy())
    names = ('3-4', '1-2')
    plt.figure()
    plot_confusion_matrix(confusion, 
                          names, 
                          save_plot, 
                          display_plot,
                          normalize=False)
    
    return model 

# A function to take the trained LSTM model, and use it to classify our data. 
# Funciton input arg 1: classifiation_directory [string] --> The directory containing the data which needs to be classified. 
# Function input arg 1: LSTM_model [bound method] --> The trained LSTM model from the 'train_LSTM' function. 
def classify_waves(classifiation_directory,
                   LSTM_model = trained_model):
    
    ##### (1) Extract the data from the directory. 
    df = get_red_waves(directory, train_or_classify='classify')
    
    ##### (2) Use the computer graphics card if one is available. 
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    ##### (3) Load and prepare the data. 
    x = df.iloc[:, 2:len(df.columns)]
    
    # Transpose all the data, so that the LSTM correctly understands that each column is a sequence. 
    x = x.transpose()
    
    # Scale the data by 'removing the mean and scaling to unit variance'.
    x[x.columns] = StandardScaler().fit_transform(x[x.columns])
    
    # Convert the data to tensors. 
    x = torch.from_numpy(x.to_numpy(np.float32))

    # Disable the dropout while we classify our data. 
    trained_model.eval() 

    # torch.no_grad() disables gradient calucation. We don't neet it for classification. 
    with torch.no_grad():

        predicted = trained_model(x.to(device))
        predicted_logical = predicted[:,0].round().cpu()
    
    ##### (4) Create a new directory to store the class '1' waves. 
    
    new_directory = os.path.join(classification_directory, 'grade_1_waves')
    os.mkdir(new_directory) 
    
    ##### (5) Copy class '1' wave files into the new directory. 
    
    for x in range(len(df)):
        
        if predicted_logical[x] == 1: 
        
            file_name = df.iloc[x, 1]
            current_file_path = os.path.join(classification_directory, file_name)
            new_file_path = os.path.join(new_directory, file_name)

            shutil.copyfile(current_file_path, new_file_path)
    
    ##### (6) Print completion statement. 
    
    print(f"'classify_waves' is complete. You can find the grade 1 waves in ", new_directory)